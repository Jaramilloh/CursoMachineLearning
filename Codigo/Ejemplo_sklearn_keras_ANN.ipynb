{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ejemplo_sklearn_keras_ANN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO7tprKALQhq1Hq7exTdMUI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/calderonf/CursoMachineLearning/blob/master/Codigo/Ejemplo_sklearn_keras_ANN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXWkOyLxdrWS"
      },
      "source": [
        "Ejemplo sklearn keras ANN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnkzYYkgdurm",
        "outputId": "871e4f44-5ed9-410b-a872-994b91456cc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.datasets import make_classification, make_moons,make_circles\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "verbose=True\n",
        "\n",
        "X, Y = make_circles(n_samples=500, factor=0.5, noise=0.05,random_state=5+14)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y,random_state=1)\n",
        "\n",
        "clf = MLPClassifier(hidden_layer_sizes=(8, 8),activation='relu',random_state=1, max_iter=300,verbose=verbose)\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "print(clf.predict_proba(X_test[:1]))\n",
        "\n",
        "print(clf.predict(X_test[:5, :]))\n",
        "\n",
        "print(clf.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.71034915\n",
            "Iteration 2, loss = 0.70762362\n",
            "Iteration 3, loss = 0.70553024\n",
            "Iteration 4, loss = 0.70346353\n",
            "Iteration 5, loss = 0.70144742\n",
            "Iteration 6, loss = 0.69957850\n",
            "Iteration 7, loss = 0.69784752\n",
            "Iteration 8, loss = 0.69622363\n",
            "Iteration 9, loss = 0.69451247\n",
            "Iteration 10, loss = 0.69298447\n",
            "Iteration 11, loss = 0.69161694\n",
            "Iteration 12, loss = 0.69011761\n",
            "Iteration 13, loss = 0.68882775\n",
            "Iteration 14, loss = 0.68746148\n",
            "Iteration 15, loss = 0.68627215\n",
            "Iteration 16, loss = 0.68503544\n",
            "Iteration 17, loss = 0.68385649\n",
            "Iteration 18, loss = 0.68280361\n",
            "Iteration 19, loss = 0.68162350\n",
            "Iteration 20, loss = 0.68057746\n",
            "Iteration 21, loss = 0.67947724\n",
            "Iteration 22, loss = 0.67837648\n",
            "Iteration 23, loss = 0.67738891\n",
            "Iteration 24, loss = 0.67631469\n",
            "Iteration 25, loss = 0.67521023\n",
            "Iteration 26, loss = 0.67419062\n",
            "Iteration 27, loss = 0.67318109\n",
            "Iteration 28, loss = 0.67218349\n",
            "Iteration 29, loss = 0.67111323\n",
            "Iteration 30, loss = 0.67004798\n",
            "Iteration 31, loss = 0.66906313\n",
            "Iteration 32, loss = 0.66806825\n",
            "Iteration 33, loss = 0.66695884\n",
            "Iteration 34, loss = 0.66586834\n",
            "Iteration 35, loss = 0.66484852\n",
            "Iteration 36, loss = 0.66376589\n",
            "Iteration 37, loss = 0.66270536\n",
            "Iteration 38, loss = 0.66160398\n",
            "Iteration 39, loss = 0.66051737\n",
            "Iteration 40, loss = 0.65940300\n",
            "Iteration 41, loss = 0.65827170\n",
            "Iteration 42, loss = 0.65714775\n",
            "Iteration 43, loss = 0.65602580\n",
            "Iteration 44, loss = 0.65487872\n",
            "Iteration 45, loss = 0.65370993\n",
            "Iteration 46, loss = 0.65255016\n",
            "Iteration 47, loss = 0.65136632\n",
            "Iteration 48, loss = 0.65016213\n",
            "Iteration 49, loss = 0.64893649\n",
            "Iteration 50, loss = 0.64773115\n",
            "Iteration 51, loss = 0.64653004\n",
            "Iteration 52, loss = 0.64522260\n",
            "Iteration 53, loss = 0.64394274\n",
            "Iteration 54, loss = 0.64262795\n",
            "Iteration 55, loss = 0.64133642\n",
            "Iteration 56, loss = 0.63999519\n",
            "Iteration 57, loss = 0.63864902\n",
            "Iteration 58, loss = 0.63728985\n",
            "Iteration 59, loss = 0.63592944\n",
            "Iteration 60, loss = 0.63452698\n",
            "Iteration 61, loss = 0.63304115\n",
            "Iteration 62, loss = 0.63158028\n",
            "Iteration 63, loss = 0.63001764\n",
            "Iteration 64, loss = 0.62846245\n",
            "Iteration 65, loss = 0.62676415\n",
            "Iteration 66, loss = 0.62506385\n",
            "Iteration 67, loss = 0.62332864\n",
            "Iteration 68, loss = 0.62141371\n",
            "Iteration 69, loss = 0.61949537\n",
            "Iteration 70, loss = 0.61788381\n",
            "Iteration 71, loss = 0.61616874\n",
            "Iteration 72, loss = 0.61443788\n",
            "Iteration 73, loss = 0.61282205\n",
            "Iteration 74, loss = 0.61102488\n",
            "Iteration 75, loss = 0.60935337\n",
            "Iteration 76, loss = 0.60762506\n",
            "Iteration 77, loss = 0.60586654\n",
            "Iteration 78, loss = 0.60406976\n",
            "Iteration 79, loss = 0.60232004\n",
            "Iteration 80, loss = 0.60057570\n",
            "Iteration 81, loss = 0.59876518\n",
            "Iteration 82, loss = 0.59690026\n",
            "Iteration 83, loss = 0.59495242\n",
            "Iteration 84, loss = 0.59304381\n",
            "Iteration 85, loss = 0.59102127\n",
            "Iteration 86, loss = 0.58906828\n",
            "Iteration 87, loss = 0.58710985\n",
            "Iteration 88, loss = 0.58506438\n",
            "Iteration 89, loss = 0.58301429\n",
            "Iteration 90, loss = 0.58097648\n",
            "Iteration 91, loss = 0.57881989\n",
            "Iteration 92, loss = 0.57669560\n",
            "Iteration 93, loss = 0.57459763\n",
            "Iteration 94, loss = 0.57245587\n",
            "Iteration 95, loss = 0.57024412\n",
            "Iteration 96, loss = 0.56807547\n",
            "Iteration 97, loss = 0.56585143\n",
            "Iteration 98, loss = 0.56357296\n",
            "Iteration 99, loss = 0.56131195\n",
            "Iteration 100, loss = 0.55902334\n",
            "Iteration 101, loss = 0.55666581\n",
            "Iteration 102, loss = 0.55430075\n",
            "Iteration 103, loss = 0.55195324\n",
            "Iteration 104, loss = 0.54954278\n",
            "Iteration 105, loss = 0.54711487\n",
            "Iteration 106, loss = 0.54466499\n",
            "Iteration 107, loss = 0.54219103\n",
            "Iteration 108, loss = 0.53976518\n",
            "Iteration 109, loss = 0.53720638\n",
            "Iteration 110, loss = 0.53470659\n",
            "Iteration 111, loss = 0.53201406\n",
            "Iteration 112, loss = 0.52942041\n",
            "Iteration 113, loss = 0.52677547\n",
            "Iteration 114, loss = 0.52415347\n",
            "Iteration 115, loss = 0.52152400\n",
            "Iteration 116, loss = 0.51879640\n",
            "Iteration 117, loss = 0.51609668\n",
            "Iteration 118, loss = 0.51331648\n",
            "Iteration 119, loss = 0.51052727\n",
            "Iteration 120, loss = 0.50778692\n",
            "Iteration 121, loss = 0.50497945\n",
            "Iteration 122, loss = 0.50215952\n",
            "Iteration 123, loss = 0.49929484\n",
            "Iteration 124, loss = 0.49639633\n",
            "Iteration 125, loss = 0.49354321\n",
            "Iteration 126, loss = 0.49054198\n",
            "Iteration 127, loss = 0.48760325\n",
            "Iteration 128, loss = 0.48463093\n",
            "Iteration 129, loss = 0.48158661\n",
            "Iteration 130, loss = 0.47855502\n",
            "Iteration 131, loss = 0.47548164\n",
            "Iteration 132, loss = 0.47240259\n",
            "Iteration 133, loss = 0.46936596\n",
            "Iteration 134, loss = 0.46618270\n",
            "Iteration 135, loss = 0.46304201\n",
            "Iteration 136, loss = 0.45987254\n",
            "Iteration 137, loss = 0.45670709\n",
            "Iteration 138, loss = 0.45358355\n",
            "Iteration 139, loss = 0.45031213\n",
            "Iteration 140, loss = 0.44715411\n",
            "Iteration 141, loss = 0.44399410\n",
            "Iteration 142, loss = 0.44068716\n",
            "Iteration 143, loss = 0.43748505\n",
            "Iteration 144, loss = 0.43429562\n",
            "Iteration 145, loss = 0.43107530\n",
            "Iteration 146, loss = 0.42777937\n",
            "Iteration 147, loss = 0.42457628\n",
            "Iteration 148, loss = 0.42124982\n",
            "Iteration 149, loss = 0.41800089\n",
            "Iteration 150, loss = 0.41475251\n",
            "Iteration 151, loss = 0.41138099\n",
            "Iteration 152, loss = 0.40809355\n",
            "Iteration 153, loss = 0.40478679\n",
            "Iteration 154, loss = 0.40148395\n",
            "Iteration 155, loss = 0.39808666\n",
            "Iteration 156, loss = 0.39481169\n",
            "Iteration 157, loss = 0.39149828\n",
            "Iteration 158, loss = 0.38813450\n",
            "Iteration 159, loss = 0.38477826\n",
            "Iteration 160, loss = 0.38163629\n",
            "Iteration 161, loss = 0.37818736\n",
            "Iteration 162, loss = 0.37481822\n",
            "Iteration 163, loss = 0.37157436\n",
            "Iteration 164, loss = 0.36822943\n",
            "Iteration 165, loss = 0.36491551\n",
            "Iteration 166, loss = 0.36161446\n",
            "Iteration 167, loss = 0.35832905\n",
            "Iteration 168, loss = 0.35505478\n",
            "Iteration 169, loss = 0.35181731\n",
            "Iteration 170, loss = 0.34851290\n",
            "Iteration 171, loss = 0.34523579\n",
            "Iteration 172, loss = 0.34196373\n",
            "Iteration 173, loss = 0.33873160\n",
            "Iteration 174, loss = 0.33550423\n",
            "Iteration 175, loss = 0.33223233\n",
            "Iteration 176, loss = 0.32907549\n",
            "Iteration 177, loss = 0.32586075\n",
            "Iteration 178, loss = 0.32265226\n",
            "Iteration 179, loss = 0.31958306\n",
            "Iteration 180, loss = 0.31639795\n",
            "Iteration 181, loss = 0.31325296\n",
            "Iteration 182, loss = 0.31013323\n",
            "Iteration 183, loss = 0.30701888\n",
            "Iteration 184, loss = 0.30392519\n",
            "Iteration 185, loss = 0.30091539\n",
            "Iteration 186, loss = 0.29783913\n",
            "Iteration 187, loss = 0.29482078\n",
            "Iteration 188, loss = 0.29179269\n",
            "Iteration 189, loss = 0.28884422\n",
            "Iteration 190, loss = 0.28586369\n",
            "Iteration 191, loss = 0.28286683\n",
            "Iteration 192, loss = 0.27995233\n",
            "Iteration 193, loss = 0.27698829\n",
            "Iteration 194, loss = 0.27420287\n",
            "Iteration 195, loss = 0.27118443\n",
            "Iteration 196, loss = 0.26835703\n",
            "Iteration 197, loss = 0.26557253\n",
            "Iteration 198, loss = 0.26263017\n",
            "Iteration 199, loss = 0.25982404\n",
            "Iteration 200, loss = 0.25703295\n",
            "Iteration 201, loss = 0.25446041\n",
            "Iteration 202, loss = 0.25152267\n",
            "Iteration 203, loss = 0.24880300\n",
            "Iteration 204, loss = 0.24610903\n",
            "Iteration 205, loss = 0.24346005\n",
            "Iteration 206, loss = 0.24075039\n",
            "Iteration 207, loss = 0.23810383\n",
            "Iteration 208, loss = 0.23548232\n",
            "Iteration 209, loss = 0.23286213\n",
            "Iteration 210, loss = 0.23021905\n",
            "Iteration 211, loss = 0.22763669\n",
            "Iteration 212, loss = 0.22507626\n",
            "Iteration 213, loss = 0.22252713\n",
            "Iteration 214, loss = 0.21998249\n",
            "Iteration 215, loss = 0.21744604\n",
            "Iteration 216, loss = 0.21491650\n",
            "Iteration 217, loss = 0.21248311\n",
            "Iteration 218, loss = 0.20999920\n",
            "Iteration 219, loss = 0.20757561\n",
            "Iteration 220, loss = 0.20518077\n",
            "Iteration 221, loss = 0.20287868\n",
            "Iteration 222, loss = 0.20053710\n",
            "Iteration 223, loss = 0.19827055\n",
            "Iteration 224, loss = 0.19592964\n",
            "Iteration 225, loss = 0.19369239\n",
            "Iteration 226, loss = 0.19146005\n",
            "Iteration 227, loss = 0.18920147\n",
            "Iteration 228, loss = 0.18703922\n",
            "Iteration 229, loss = 0.18488052\n",
            "Iteration 230, loss = 0.18273180\n",
            "Iteration 231, loss = 0.18061533\n",
            "Iteration 232, loss = 0.17853688\n",
            "Iteration 233, loss = 0.17646663\n",
            "Iteration 234, loss = 0.17444991\n",
            "Iteration 235, loss = 0.17244171\n",
            "Iteration 236, loss = 0.17045076\n",
            "Iteration 237, loss = 0.16855989\n",
            "Iteration 238, loss = 0.16658652\n",
            "Iteration 239, loss = 0.16464686\n",
            "Iteration 240, loss = 0.16275338\n",
            "Iteration 241, loss = 0.16085771\n",
            "Iteration 242, loss = 0.15902663\n",
            "Iteration 243, loss = 0.15720887\n",
            "Iteration 244, loss = 0.15536539\n",
            "Iteration 245, loss = 0.15357862\n",
            "Iteration 246, loss = 0.15183490\n",
            "Iteration 247, loss = 0.15007342\n",
            "Iteration 248, loss = 0.14835097\n",
            "Iteration 249, loss = 0.14667323\n",
            "Iteration 250, loss = 0.14497832\n",
            "Iteration 251, loss = 0.14329875\n",
            "Iteration 252, loss = 0.14166659\n",
            "Iteration 253, loss = 0.14005907\n",
            "Iteration 254, loss = 0.13843425\n",
            "Iteration 255, loss = 0.13685395\n",
            "Iteration 256, loss = 0.13526577\n",
            "Iteration 257, loss = 0.13372777\n",
            "Iteration 258, loss = 0.13217833\n",
            "Iteration 259, loss = 0.13068212\n",
            "Iteration 260, loss = 0.12916850\n",
            "Iteration 261, loss = 0.12768649\n",
            "Iteration 262, loss = 0.12618669\n",
            "Iteration 263, loss = 0.12478128\n",
            "Iteration 264, loss = 0.12333871\n",
            "Iteration 265, loss = 0.12192118\n",
            "Iteration 266, loss = 0.12050470\n",
            "Iteration 267, loss = 0.11912205\n",
            "Iteration 268, loss = 0.11775146\n",
            "Iteration 269, loss = 0.11641497\n",
            "Iteration 270, loss = 0.11503645\n",
            "Iteration 271, loss = 0.11372867\n",
            "Iteration 272, loss = 0.11243377\n",
            "Iteration 273, loss = 0.11112242\n",
            "Iteration 274, loss = 0.10986492\n",
            "Iteration 275, loss = 0.10858014\n",
            "Iteration 276, loss = 0.10736233\n",
            "Iteration 277, loss = 0.10615145\n",
            "Iteration 278, loss = 0.10493233\n",
            "Iteration 279, loss = 0.10372426\n",
            "Iteration 280, loss = 0.10255987\n",
            "Iteration 281, loss = 0.10139348\n",
            "Iteration 282, loss = 0.10025399\n",
            "Iteration 283, loss = 0.09909822\n",
            "Iteration 284, loss = 0.09799279\n",
            "Iteration 285, loss = 0.09688974\n",
            "Iteration 286, loss = 0.09579648\n",
            "Iteration 287, loss = 0.09472771\n",
            "Iteration 288, loss = 0.09368241\n",
            "Iteration 289, loss = 0.09258924\n",
            "Iteration 290, loss = 0.09155898\n",
            "Iteration 291, loss = 0.09054639\n",
            "Iteration 292, loss = 0.08956325\n",
            "Iteration 293, loss = 0.08855384\n",
            "Iteration 294, loss = 0.08757011\n",
            "Iteration 295, loss = 0.08661062\n",
            "Iteration 296, loss = 0.08565892\n",
            "Iteration 297, loss = 0.08473106\n",
            "Iteration 298, loss = 0.08381836\n",
            "Iteration 299, loss = 0.08288489\n",
            "Iteration 300, loss = 0.08198136\n",
            "[[0.94871648 0.05128352]]\n",
            "[0 1 0 0 1]\n",
            "1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAeA2RWEjnRr",
        "outputId": "20f02487-05d6-44bb-b6ff-1e360ab65761",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as kr\n",
        "\n",
        "from IPython.core.display import display, HTML\n",
        "\n",
        "lr = 0.01           # learning rate\n",
        "nn = [2, 8, 8, 1]  # número de neuronas por capa. note que acá están las de entrada y salida, no solo las ocultas\n",
        "\n",
        "# Creamos el objeto que contendrá a nuestra red neuronal, como\n",
        "# secuencia de capas.\n",
        "model = kr.Sequential()\n",
        "\n",
        "# Añadimos la capa 1\n",
        "l1 = model.add(kr.layers.Dense(nn[1], activation='relu'))\n",
        "\n",
        "# Añadimos la capa 2\n",
        "l2 = model.add(kr.layers.Dense(nn[2], activation='relu'))\n",
        "\n",
        "# Añadimos la capa 3\n",
        "l3 = model.add(kr.layers.Dense(nn[3], activation='sigmoid'))\n",
        "\n",
        "# Compilamos el modelo, definiendo la función de coste y el optimizador.\n",
        "model.compile(loss='mse', optimizer=kr.optimizers.SGD(lr=0.05), metrics=['acc'])\n",
        "\n",
        "# Y entrenamos al modelo. Los callbacks \n",
        "model.fit(X, Y, epochs=200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2452 - acc: 0.5120\n",
            "Epoch 2/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2439 - acc: 0.5140\n",
            "Epoch 3/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2426 - acc: 0.5180\n",
            "Epoch 4/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2414 - acc: 0.5300\n",
            "Epoch 5/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2402 - acc: 0.5440\n",
            "Epoch 6/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2390 - acc: 0.5520\n",
            "Epoch 7/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2378 - acc: 0.5680\n",
            "Epoch 8/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2366 - acc: 0.5540\n",
            "Epoch 9/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2353 - acc: 0.5800\n",
            "Epoch 10/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2342 - acc: 0.5980\n",
            "Epoch 11/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2329 - acc: 0.5900\n",
            "Epoch 12/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2316 - acc: 0.5900\n",
            "Epoch 13/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2303 - acc: 0.6180\n",
            "Epoch 14/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2290 - acc: 0.6280\n",
            "Epoch 15/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2276 - acc: 0.6540\n",
            "Epoch 16/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2263 - acc: 0.6880\n",
            "Epoch 17/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2249 - acc: 0.7300\n",
            "Epoch 18/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2234 - acc: 0.7240\n",
            "Epoch 19/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2220 - acc: 0.7400\n",
            "Epoch 20/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2205 - acc: 0.7300\n",
            "Epoch 21/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.2190 - acc: 0.7320\n",
            "Epoch 22/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2174 - acc: 0.7580\n",
            "Epoch 23/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2157 - acc: 0.7860\n",
            "Epoch 24/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2140 - acc: 0.8000\n",
            "Epoch 25/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2120 - acc: 0.8200\n",
            "Epoch 26/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2098 - acc: 0.8160\n",
            "Epoch 27/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2073 - acc: 0.8540\n",
            "Epoch 28/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2049 - acc: 0.8340\n",
            "Epoch 29/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.2023 - acc: 0.8580\n",
            "Epoch 30/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.1996 - acc: 0.8780\n",
            "Epoch 31/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.1969 - acc: 0.8820\n",
            "Epoch 32/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.1941 - acc: 0.8860\n",
            "Epoch 33/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.1911 - acc: 0.8900\n",
            "Epoch 34/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.1880 - acc: 0.8920\n",
            "Epoch 35/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.1850 - acc: 0.8940\n",
            "Epoch 36/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.1818 - acc: 0.9020\n",
            "Epoch 37/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.1786 - acc: 0.9020\n",
            "Epoch 38/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.1755 - acc: 0.9040\n",
            "Epoch 39/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.1723 - acc: 0.9100\n",
            "Epoch 40/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.1690 - acc: 0.9100\n",
            "Epoch 41/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.1656 - acc: 0.9160\n",
            "Epoch 42/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.1624 - acc: 0.9180\n",
            "Epoch 43/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.1588 - acc: 0.9260\n",
            "Epoch 44/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.1553 - acc: 0.9300\n",
            "Epoch 45/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.1517 - acc: 0.9340\n",
            "Epoch 46/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.1482 - acc: 0.9360\n",
            "Epoch 47/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.1445 - acc: 0.9400\n",
            "Epoch 48/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.1407 - acc: 0.9440\n",
            "Epoch 49/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.1368 - acc: 0.9500\n",
            "Epoch 50/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.1330 - acc: 0.9560\n",
            "Epoch 51/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.1291 - acc: 0.9600\n",
            "Epoch 52/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.1251 - acc: 0.9660\n",
            "Epoch 53/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.1212 - acc: 0.9740\n",
            "Epoch 54/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.1172 - acc: 0.9760\n",
            "Epoch 55/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.1133 - acc: 0.9820\n",
            "Epoch 56/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.1092 - acc: 0.9900\n",
            "Epoch 57/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.1052 - acc: 0.9940\n",
            "Epoch 58/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.1016 - acc: 0.9960\n",
            "Epoch 59/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0977 - acc: 1.0000\n",
            "Epoch 60/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0940 - acc: 1.0000\n",
            "Epoch 61/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0903 - acc: 1.0000\n",
            "Epoch 62/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0868 - acc: 1.0000\n",
            "Epoch 63/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0833 - acc: 1.0000\n",
            "Epoch 64/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0800 - acc: 1.0000\n",
            "Epoch 65/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0767 - acc: 1.0000\n",
            "Epoch 66/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0736 - acc: 1.0000\n",
            "Epoch 67/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0706 - acc: 1.0000\n",
            "Epoch 68/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0676 - acc: 1.0000\n",
            "Epoch 69/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0648 - acc: 1.0000\n",
            "Epoch 70/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0621 - acc: 1.0000\n",
            "Epoch 71/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0594 - acc: 1.0000\n",
            "Epoch 72/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0568 - acc: 1.0000\n",
            "Epoch 73/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0544 - acc: 1.0000\n",
            "Epoch 74/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0521 - acc: 1.0000\n",
            "Epoch 75/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0498 - acc: 1.0000\n",
            "Epoch 76/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0477 - acc: 1.0000\n",
            "Epoch 77/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0457 - acc: 1.0000\n",
            "Epoch 78/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0438 - acc: 1.0000\n",
            "Epoch 79/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0420 - acc: 1.0000\n",
            "Epoch 80/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0404 - acc: 1.0000\n",
            "Epoch 81/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0386 - acc: 1.0000\n",
            "Epoch 82/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0371 - acc: 1.0000\n",
            "Epoch 83/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0356 - acc: 1.0000\n",
            "Epoch 84/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0343 - acc: 1.0000\n",
            "Epoch 85/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0329 - acc: 1.0000\n",
            "Epoch 86/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0317 - acc: 1.0000\n",
            "Epoch 87/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0305 - acc: 1.0000\n",
            "Epoch 88/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0293 - acc: 1.0000\n",
            "Epoch 89/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0283 - acc: 1.0000\n",
            "Epoch 90/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0272 - acc: 1.0000\n",
            "Epoch 91/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0262 - acc: 1.0000\n",
            "Epoch 92/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0254 - acc: 1.0000\n",
            "Epoch 93/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0245 - acc: 1.0000\n",
            "Epoch 94/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0237 - acc: 1.0000\n",
            "Epoch 95/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0229 - acc: 1.0000\n",
            "Epoch 96/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0222 - acc: 1.0000\n",
            "Epoch 97/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0215 - acc: 1.0000\n",
            "Epoch 98/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0208 - acc: 1.0000\n",
            "Epoch 99/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0201 - acc: 1.0000\n",
            "Epoch 100/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0196 - acc: 1.0000\n",
            "Epoch 101/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0190 - acc: 1.0000\n",
            "Epoch 102/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0184 - acc: 1.0000\n",
            "Epoch 103/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0179 - acc: 1.0000\n",
            "Epoch 104/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0174 - acc: 1.0000\n",
            "Epoch 105/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0169 - acc: 1.0000\n",
            "Epoch 106/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0165 - acc: 1.0000\n",
            "Epoch 107/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0160 - acc: 1.0000\n",
            "Epoch 108/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0156 - acc: 1.0000\n",
            "Epoch 109/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0152 - acc: 1.0000\n",
            "Epoch 110/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0148 - acc: 1.0000\n",
            "Epoch 111/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0144 - acc: 1.0000\n",
            "Epoch 112/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0141 - acc: 1.0000\n",
            "Epoch 113/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0137 - acc: 1.0000\n",
            "Epoch 114/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0134 - acc: 1.0000\n",
            "Epoch 115/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0131 - acc: 1.0000\n",
            "Epoch 116/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0128 - acc: 1.0000\n",
            "Epoch 117/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0125 - acc: 1.0000\n",
            "Epoch 118/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0122 - acc: 1.0000\n",
            "Epoch 119/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0119 - acc: 1.0000\n",
            "Epoch 120/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0116 - acc: 1.0000\n",
            "Epoch 121/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0114 - acc: 1.0000\n",
            "Epoch 122/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0111 - acc: 1.0000\n",
            "Epoch 123/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0109 - acc: 1.0000\n",
            "Epoch 124/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0107 - acc: 1.0000\n",
            "Epoch 125/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0104 - acc: 1.0000\n",
            "Epoch 126/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0102 - acc: 1.0000\n",
            "Epoch 127/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0100 - acc: 1.0000\n",
            "Epoch 128/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0098 - acc: 1.0000\n",
            "Epoch 129/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0096 - acc: 1.0000\n",
            "Epoch 130/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0095 - acc: 1.0000\n",
            "Epoch 131/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0093 - acc: 1.0000\n",
            "Epoch 132/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0091 - acc: 1.0000\n",
            "Epoch 133/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0089 - acc: 1.0000\n",
            "Epoch 134/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0088 - acc: 1.0000\n",
            "Epoch 135/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0086 - acc: 1.0000\n",
            "Epoch 136/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0084 - acc: 1.0000\n",
            "Epoch 137/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0083 - acc: 1.0000\n",
            "Epoch 138/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0082 - acc: 1.0000\n",
            "Epoch 139/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0080 - acc: 1.0000\n",
            "Epoch 140/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0079 - acc: 1.0000\n",
            "Epoch 141/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0077 - acc: 1.0000\n",
            "Epoch 142/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0076 - acc: 1.0000\n",
            "Epoch 143/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0075 - acc: 1.0000\n",
            "Epoch 144/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0073 - acc: 1.0000\n",
            "Epoch 145/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0072 - acc: 1.0000\n",
            "Epoch 146/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0071 - acc: 1.0000\n",
            "Epoch 147/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0070 - acc: 1.0000\n",
            "Epoch 148/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0069 - acc: 1.0000\n",
            "Epoch 149/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0068 - acc: 1.0000\n",
            "Epoch 150/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0067 - acc: 1.0000\n",
            "Epoch 151/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0066 - acc: 1.0000\n",
            "Epoch 152/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0065 - acc: 1.0000\n",
            "Epoch 153/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0064 - acc: 1.0000\n",
            "Epoch 154/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0063 - acc: 1.0000\n",
            "Epoch 155/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0062 - acc: 1.0000\n",
            "Epoch 156/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0061 - acc: 1.0000\n",
            "Epoch 157/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0060 - acc: 1.0000\n",
            "Epoch 158/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0059 - acc: 1.0000\n",
            "Epoch 159/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0058 - acc: 1.0000\n",
            "Epoch 160/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0058 - acc: 1.0000\n",
            "Epoch 161/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0057 - acc: 1.0000\n",
            "Epoch 162/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0056 - acc: 1.0000\n",
            "Epoch 163/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0055 - acc: 1.0000\n",
            "Epoch 164/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0054 - acc: 1.0000\n",
            "Epoch 165/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0054 - acc: 1.0000\n",
            "Epoch 166/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0053 - acc: 1.0000\n",
            "Epoch 167/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0052 - acc: 1.0000\n",
            "Epoch 168/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0052 - acc: 1.0000\n",
            "Epoch 169/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0051 - acc: 1.0000\n",
            "Epoch 170/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0050 - acc: 1.0000\n",
            "Epoch 171/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0050 - acc: 1.0000\n",
            "Epoch 172/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0049 - acc: 1.0000\n",
            "Epoch 173/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0049 - acc: 1.0000\n",
            "Epoch 174/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0048 - acc: 1.0000\n",
            "Epoch 175/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0047 - acc: 1.0000\n",
            "Epoch 176/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0047 - acc: 1.0000\n",
            "Epoch 177/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0046 - acc: 1.0000\n",
            "Epoch 178/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0046 - acc: 1.0000\n",
            "Epoch 179/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0045 - acc: 1.0000\n",
            "Epoch 180/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0045 - acc: 1.0000\n",
            "Epoch 181/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0044 - acc: 1.0000\n",
            "Epoch 182/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0044 - acc: 1.0000\n",
            "Epoch 183/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0043 - acc: 1.0000\n",
            "Epoch 184/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0043 - acc: 1.0000\n",
            "Epoch 185/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0042 - acc: 1.0000\n",
            "Epoch 186/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0042 - acc: 1.0000\n",
            "Epoch 187/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0041 - acc: 1.0000\n",
            "Epoch 188/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0041 - acc: 1.0000\n",
            "Epoch 189/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0040 - acc: 1.0000\n",
            "Epoch 190/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0040 - acc: 1.0000\n",
            "Epoch 191/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0040 - acc: 1.0000\n",
            "Epoch 192/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0039 - acc: 1.0000\n",
            "Epoch 193/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0039 - acc: 1.0000\n",
            "Epoch 194/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0038 - acc: 1.0000\n",
            "Epoch 195/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0038 - acc: 1.0000\n",
            "Epoch 196/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0037 - acc: 1.0000\n",
            "Epoch 197/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0037 - acc: 1.0000\n",
            "Epoch 198/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0037 - acc: 1.0000\n",
            "Epoch 199/200\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0036 - acc: 1.0000\n",
            "Epoch 200/200\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0036 - acc: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f15abdefb70>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    }
  ]
}